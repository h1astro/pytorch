{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.randn(5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9586, -1.2168, -0.5338],\n",
       "        [-0.9936,  0.5299, -0.0387],\n",
       "        [-0.4098,  1.5394,  0.0322],\n",
       "        [ 1.3615,  0.2598,  0.9574],\n",
       "        [-0.7260,  0.7264,  0.3477]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9586, -1.2168, -0.5338],\n",
       "        [-0.9936,  0.5299, -0.0387],\n",
       "        [-0.4098,  1.5394,  0.0322],\n",
       "        [ 1.3615,  0.2598,  0.9574],\n",
       "        [-0.7260,  0.7264,  0.3477]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=x[0:]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.9586, -0.2168,  0.4662],\n",
       "        [ 0.0064,  1.5299,  0.9613],\n",
       "        [ 0.5902,  2.5394,  1.0322],\n",
       "        [ 2.3615,  1.2598,  1.9574],\n",
       "        [ 0.2740,  1.7264,  1.3477]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x# 源tensor也被改了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 改变形状\n",
    "用`view()`来改变`Tensor`的形状："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3]) torch.Size([15]) torch.Size([3, 5])\n"
     ]
    }
   ],
   "source": [
    "y = x.view(15)\n",
    "z = x.view(-1, 5)  # -1所指的维度可以根据其他维度的值推出来\n",
    "print(x.size(), y.size(), z.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意`view()`返回的新tensor与源tensor共享内存（其实是同一个tensor），也即更改其中的一个，另外一个也会跟着改变。(顾名思义，view仅仅是改变了对这个张量的观察角度)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.9586, 1.7832, 2.4662],\n",
      "        [2.0064, 3.5299, 2.9613],\n",
      "        [2.5902, 4.5394, 3.0322],\n",
      "        [4.3615, 3.2598, 3.9574],\n",
      "        [2.2740, 3.7264, 3.3477]])\n",
      "tensor([3.9586, 1.7832, 2.4662, 2.0064, 3.5299, 2.9613, 2.5902, 4.5394, 3.0322,\n",
      "        4.3615, 3.2598, 3.9574, 2.2740, 3.7264, 3.3477])\n"
     ]
    }
   ],
   "source": [
    "x+=1\n",
    "print(x)\n",
    "print(y)# y也+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以如果我们想返回一个真正新的副本（即不共享内存）该怎么办呢？Pytorch还提供了一个`reshape()`可以改变形状，但是此函数并不能保证返回的是其拷贝，所以不推荐使用。推荐先用`clone`创造一个副本然后再使用`view`。[参考此处]("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cp=x.clone().view(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.9586, 1.7832, 2.4662, 2.0064, 3.5299, 2.9613, 2.5902, 4.5394, 3.0322,\n",
       "        4.3615, 3.2598, 3.9574, 2.2740, 3.7264, 3.3477])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.9586, 0.7832, 1.4662],\n",
      "        [1.0064, 2.5299, 1.9613],\n",
      "        [1.5902, 3.5394, 2.0322],\n",
      "        [3.3615, 2.2598, 2.9574],\n",
      "        [1.2740, 2.7264, 2.3477]])\n",
      "tensor([3.9586, 1.7832, 2.4662, 2.0064, 3.5299, 2.9613, 2.5902, 4.5394, 3.0322,\n",
      "        4.3615, 3.2598, 3.9574, 2.2740, 3.7264, 3.3477])\n"
     ]
    }
   ],
   "source": [
    "x-=1\n",
    "print(x)\n",
    "print(x_cp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 使用`clone`还有一个好处是会被记录在计算图中，即梯度回传到副本时也会传到源`Tensor`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另外一个常用的函数就是`item()`, 它可以将一个标量`Tensor`转换成一个Python number："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0628])\n",
      "0.06281913816928864\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1)\n",
    "print(x)\n",
    "print(x.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.3 广播机制\n",
    "\n",
    "前面我们看到如何对两个形状相同的`Tensor`做按元素运算。当对两个形状不同的`Tensor`按元素运算时，可能会触发广播（broadcasting）机制：先适当复制元素使这两个`Tensor`形状相同后再按元素运算。例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2]])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "tensor([[2, 3],\n",
      "        [3, 4],\n",
      "        [4, 5]])\n"
     ]
    }
   ],
   "source": [
    "x=torch.arange(1,3).view(1,2)\n",
    "print(x)\n",
    "y=torch.arange(1,4).view(3,1)\n",
    "print(y)\n",
    "print(x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于`x`和`y`分别是1行2列和3行1列的矩阵，如果要计算`x + y`，那么`x`中第一行的2个元素被广播（复制）到了第二行和第三行，而`y`中第一列的3个元素被广播（复制）到了第二列。如此，就可以对2个3行2列的矩阵按元素相加。\n",
    "\n",
    "## 2.2.4 运算的内存开销\n",
    "\n",
    "前面说了，索引、`view`是不会开辟新内存的，而像`y = x + y`这样的运算是会新开内存的，然后将`y`指向新内存。为了演示这一点，我们可以使用Python自带的`id`函数：如果两个实例的ID一致，那么它们所对应的内存地址相同；反之则不同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "x=torch.tensor([1,2])\n",
    "y=torch.tensor([3,4])\n",
    "id_before=id(y)\n",
    "y=y+x\n",
    "print(id(y)==id_before)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果想指定结果到原来的`y`的内存，我们可以使用前面介绍的索引来进行替换操作。在下面的例子中，我们把`x + y`的结果通过`[:]`写进`y`对应的内存中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2])\n",
    "y = torch.tensor([3, 4])\n",
    "id_before = id(y)\n",
    "y[:] = y + x         #  y[:]\n",
    "print(id(y) == id_before) # True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们还可以使用运算符全名函数中的`out`参数或者自加运算符`+=`(也即`add_()`)达到上述效果，例如`torch.add(x, y, out=y)`和`y += x`(`y.add_(x)`)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2])\n",
    "y = torch.tensor([3, 4])\n",
    "id_before = id(y)\n",
    "torch.add(x, y, out=y) # y += x, y.add_(x)\n",
    "print(id(y) == id_before) # True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 所以只有y=y+x会导致重新开辟内存?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.5 `Tensor`和NumPy相互转换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们很容易用`numpy()`和`from_numpy()`将`Tensor`和NumPy中的数组相互转换。但是需要注意的一点是：\n",
    "**这两个函数所产生的的`Tensor`和NumPy中的数组共享相同的内存（所以他们之间的转换很快），改变其中一个时另一个也会改变！！！**\n",
    "\n",
    "> 还有一个常用的将NumPy中的array转换成`Tensor`的方法就是`torch.tensor()`, 需要注意的是，此方法总是会进行数据拷贝（就会消耗更多的时间和空间），所以返回的`Tensor`和原来的数据不再共享内存。\n",
    "\n",
    "### `Tensor`转NumPy\n",
    "\n",
    "使用`numpy()`将`Tensor`转换成NumPy数组:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.]) [1. 1. 1. 1. 1.]\n",
      "tensor([2., 2., 2., 2., 2.]) [2. 2. 2. 2. 2.]\n",
      "tensor([3., 3., 3., 3., 3.]) [3. 3. 3. 3. 3.]\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "b = a.numpy()\n",
    "print(a, b)\n",
    "\n",
    "a += 1\n",
    "print(a, b)\n",
    "b += 1\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy数组转`Tensor`\n",
    "\n",
    "使用`from_numpy()`将NumPy数组转换成`Tensor`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.] tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n",
      "[2. 2. 2. 2. 2.] tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
      "[3. 3. 3. 3. 3.] tensor([3., 3., 3., 3., 3.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a=np.ones(5)\n",
    "b=torch.from_numpy(a)\n",
    "print(a,b)\n",
    "\n",
    "a += 1\n",
    "print(a, b)\n",
    "b += 1\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所有在CPU上的`Tensor`（除了`CharTensor`）都支持与NumPy数组相互转换。\n",
    "\n",
    "此外上面提到还有一个常用的方法就是直接用`torch.tensor()`将NumPy数组转换成`Tensor`，需要注意的是该方法总是会进行数据拷贝，返回的`Tensor`和原来的数据不再共享内存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4. 4. 4. 4. 4.] tensor([3., 3., 3., 3., 3.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "c = torch.tensor(a)\n",
    "a += 1\n",
    "print(a, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.6 `Tensor` on GPU\n",
    "\n",
    "用方法`to()`可以将`Tensor`在CPU和GPU（需要硬件支持）之间相互移动。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以下代码只有在PyTorch GPU版本上才会执行\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # GPU\n",
    "    y = torch.ones_like(x, device=device)  # 直接创建一个在GPU上的Tensor\n",
    "    x = x.to(device)                       # 等价于 .to(\"cuda\")\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # to()还可以同时更改数据类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
